{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06943996",
   "metadata": {},
   "source": [
    "## Preprocess Data for VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e38f7f",
   "metadata": {},
   "source": [
    "The aim of this notebook is to translate NetCDF files (.nc) of three daily climate variables (maximum temperature, precipitations, wind) to a numpy 3D-array. This output array can easily be read for training and evaluating the Convolutional Variational AutoEncoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd6bc4",
   "metadata": {},
   "source": [
    "#### 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cef226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cftime\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38532b17",
   "metadata": {},
   "source": [
    "#### 1. Load Data to xarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1871dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical Datasets\n",
    "# regrouped by climate variable\n",
    "\n",
    "temp_50 = xr.open_dataset(\"../data/tasmax_day_CMCC-ESM2_historical_r1i1p1f1_gn_19500101-19741231.nc\")\n",
    "temp_75 = xr.open_dataset(\"../data/tasmax_day_CMCC-ESM2_historical_r1i1p1f1_gn_19750101-19991231.nc\")\n",
    "temp_00 = xr.open_dataset(\"../data/tasmax_day_CMCC-ESM2_historical_r1i1p1f1_gn_20000101-20141231.nc\")\n",
    "temp_histo = xr.concat([temp_50, temp_75], \"time\")\n",
    "temp_histo = xr.concat([temp_histo, temp_00], \"time\")\n",
    "\n",
    "prcp_50 = xr.open_dataset(\"../data/pr_day_CMCC-ESM2_historical_r1i1p1f1_gn_19500101-19741231.nc\")\n",
    "prcp_75 = xr.open_dataset(\"../data/pr_day_CMCC-ESM2_historical_r1i1p1f1_gn_19750101-19991231.nc\")\n",
    "prcp_00 = xr.open_dataset(\"../data/pr_day_CMCC-ESM2_historical_r1i1p1f1_gn_20000101-20141231.nc\")\n",
    "prcp_histo = xr.concat([prcp_50, prcp_75], \"time\")\n",
    "prcp_histo = xr.concat([prcp_histo, prcp_00], \"time\")\n",
    "\n",
    "wind_50 = xr.open_dataset(\"../data/sfcWind_day_CMCC-ESM2_historical_r1i1p1f1_gn_19500101-19741231.nc\")\n",
    "wind_75 = xr.open_dataset(\"../data/sfcWind_day_CMCC-ESM2_historical_r1i1p1f1_gn_19750101-19991231.nc\")\n",
    "wind_00 = xr.open_dataset(\"../data/sfcWind_day_CMCC-ESM2_historical_r1i1p1f1_gn_20000101-20141231.nc\")\n",
    "wind_histo = xr.concat([wind_50, wind_75], \"time\")\n",
    "wind_histo = xr.concat([wind_histo, wind_00], \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf840d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection Datasets\n",
    "# regrouped by climate variable\n",
    "\n",
    "temp_40 = xr.open_dataset(\"../data/tasmax_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20400101-20641231.nc\")\n",
    "temp_65 = xr.open_dataset(\"../data/tasmax_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20650101-20891231.nc\")\n",
    "temp_90 = xr.open_dataset(\"../data/tasmax_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20900101-21001231.nc\")\n",
    "temp_proj = xr.concat([temp_40, temp_65], \"time\")\n",
    "temp_proj = xr.concat([temp_proj, temp_90], \"time\")\n",
    "\n",
    "prcp_40 = xr.open_dataset(\"../data/pr_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20400101-20641231.nc\")\n",
    "prcp_65 = xr.open_dataset(\"../data/pr_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20650101-20891231.nc\")\n",
    "prcp_90 = xr.open_dataset(\"../data/pr_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20900101-21001231.nc\")\n",
    "prcp_proj = xr.concat([prcp_40, prcp_65], \"time\")\n",
    "prcp_proj = xr.concat([prcp_proj, prcp_90], \"time\")\n",
    "\n",
    "wind_40 = xr.open_dataset(\"../data/sfcWind_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20400101-20641231.nc\")\n",
    "wind_65 = xr.open_dataset(\"../data/sfcWind_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20650101-20891231.nc\")\n",
    "wind_90 = xr.open_dataset(\"../data/sfcWind_day_CMCC-ESM2_ssp585_r1i1p1f1_gn_20900101-21001231.nc\")\n",
    "wind_proj = xr.concat([wind_40, wind_65], \"time\")\n",
    "wind_proj = xr.concat([wind_proj, wind_90], \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636e761",
   "metadata": {},
   "source": [
    "#### 2. Restrict to a Geospatial Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de4410c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq32_west_europe = {\n",
    "    \"min_lon\": -10,\n",
    "    \"max_lon\": 29,\n",
    "    \"min_lat\": 36,\n",
    "    \"max_lat\": 66\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34e2ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_to_ndarray(xr_dset: xr.Dataset, \n",
    "                  sq_coords: dict\n",
    "                 ) -> (np.ndarray, np.array, str):\n",
    "    \"\"\"\n",
    "    Convert xarray dataset it to a cropped square ndarray.\n",
    "    :param sq_coords: spatial coordinates of the crop\n",
    "    \"\"\"\n",
    "    xr_dset.coords['lon'] = (xr_dset.coords['lon'] + 180) % 360 - 180\n",
    "    xr_dset = xr_dset.sortby(xr_dset.lon)\n",
    "    xr_dset = xr_dset.sel(\n",
    "        lon = slice(sq_coords['min_lon'], sq_coords['max_lon']),\n",
    "        lat = slice(sq_coords['min_lat'], sq_coords['max_lat'])\n",
    "    )\n",
    "    time_list = np.array(xr_dset['time'])\n",
    "    n_t = len(time_list)\n",
    "    n_lat = len(xr_dset.coords['lat'])\n",
    "    n_lon = len(xr_dset.coords['lon'])\n",
    "    nd_dset = np.ndarray((n_t, n_lat, n_lon, 1), dtype=\"float32\")\n",
    "    climate_variable = xr_dset.attrs['variable_id']\n",
    "    nd_dset[:, :, :, 0] = xr_dset[climate_variable][:, :, :]\n",
    "    nd_dset = np.flip(nd_dset, axis=1)\n",
    "    \n",
    "    return nd_dset, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f378d8",
   "metadata": {},
   "source": [
    "#### 3. Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f3d8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extrema(histo_dataset: np.ndarray,\n",
    "                proj_dataset: np.ndarray) -> np.array:\n",
    "    # compute global extrema over past and future\n",
    "    global_min = min(np.min(histo_dataset), np.min(proj_dataset))\n",
    "    global_max = max(np.max(histo_dataset), np.max(proj_dataset))\n",
    "    return np.array([global_min, global_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95dd5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(nd_dset: np.ndarray, \n",
    "              extrema: np.array\n",
    "             ) -> np.ndarray:\n",
    "    norm_dset = (nd_dset-extrema[0])/(extrema[1]-extrema[0])\n",
    "    return norm_dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6da4f",
   "metadata": {},
   "source": [
    "#### 4. Split Historical Data into Train and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa427d43",
   "metadata": {},
   "source": [
    "Train the network on most of the historical data, but keep some to test the model performance on new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f905b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(nd_dset: np.ndarray,\n",
    "                     time_list: np.array,\n",
    "                     train_proportion: float = 0.8\n",
    "                    ) -> (np.ndarray, np.ndarray, np.array, np.array):\n",
    "    len_train = int(len(nd_dset)*train_proportion)\n",
    "    train_data = nd_dset[:len_train]\n",
    "    test_data = nd_dset[len_train:]\n",
    "    train_time = time_list[:len_train]\n",
    "    test_time = time_list[len_train:]\n",
    "    return train_data, test_data, train_time, test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5ecab",
   "metadata": {},
   "source": [
    "#### 5. Combine into a 3D-Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feea5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndarray_to_3d(temp_dset: np.ndarray,\n",
    "                 prcp_dset: np.ndarray,\n",
    "                 wind_dset: np.ndarray\n",
    "                 ) -> np.ndarray:\n",
    "    \n",
    "    n_t = np.shape(temp_dset)[0]\n",
    "    n_lat = np.shape(temp_dset)[1]\n",
    "    n_lon = np.shape(temp_dset)[2]\n",
    "    \n",
    "    # combine all variables on a same period to a new 3D-array\n",
    "    total_dset = np.zeros((n_t, n_lat, n_lon, 3), dtype=\"float32\")\n",
    "    total_dset[:,:,:,0] = temp_dset.reshape(n_t,n_lat,n_lon)\n",
    "    total_dset[:,:,:,1] = prcp_dset.reshape(n_t,n_lat,n_lon)\n",
    "    total_dset[:,:,:,2] = wind_dset.reshape(n_t,n_lat,n_lon)\n",
    "    \n",
    "    return total_dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f08e1",
   "metadata": {},
   "source": [
    "#### 6. Full Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bb4c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_3d(temp_histo: xr.Dataset,\n",
    "                 temp_proj: xr.Dataset,\n",
    "                 prcp_histo: xr.Dataset,\n",
    "                 prcp_proj: xr.Dataset,\n",
    "                 wind_histo: xr.Dataset,\n",
    "                 wind_proj: xr.Dataset,\n",
    "                 saving_on: bool = True):\n",
    "    \n",
    "    # convert historical xarrays to ndarrays for each climate variable\n",
    "    temp_histo_nd, time_list = xr_to_ndarray(temp_histo, sq32_west_europe)\n",
    "    prcp_histo_nd, _ = xr_to_ndarray(prcp_histo, sq32_west_europe)\n",
    "    wind_histo_nd, _ = xr_to_ndarray(wind_histo, sq32_west_europe)\n",
    "\n",
    "    # projection xarrays to ndarrays\n",
    "    temp_proj_nd, time_proj = xr_to_ndarray(temp_proj, sq32_west_europe)\n",
    "    prcp_proj_nd, _ = xr_to_ndarray(prcp_proj, sq32_west_europe)\n",
    "    wind_proj_nd, _ = xr_to_ndarray(wind_proj, sq32_west_europe)\n",
    "\n",
    "    # compute extrema for each variable\n",
    "    temp_extrema = get_extrema(temp_histo_nd, temp_proj_nd)\n",
    "    prcp_extrema = get_extrema(prcp_histo_nd, prcp_proj_nd)\n",
    "    wind_extrema = get_extrema(wind_histo_nd, wind_proj_nd)\n",
    "    extrema = np.array([temp_extrema, prcp_extrema, wind_extrema, temp_extrema, prcp_extrema, wind_extrema])\n",
    "\n",
    "    # normalize all datasets\n",
    "    temp_histo_norm = normalize(temp_histo_nd, temp_extrema)\n",
    "    temp_proj_norm = normalize(temp_proj_nd, temp_extrema)\n",
    "    prcp_histo_norm = normalize(prcp_histo_nd, prcp_extrema)\n",
    "    prcp_proj_norm = normalize(prcp_proj_nd, prcp_extrema)\n",
    "    wind_histo_norm = normalize(wind_histo_nd, wind_extrema)\n",
    "    wind_proj_norm = normalize(wind_proj_nd, wind_extrema)\n",
    "\n",
    "    # split historical datasets into train and test ones\n",
    "    train_temp, test_temp, train_time, test_time = split_train_test(temp_histo_norm,\n",
    "                                                                    time_list)\n",
    "    train_prcp, test_prcp, _, _ = split_train_test(prcp_histo_norm, \n",
    "                                                   time_list)\n",
    "    train_wind, test_wind, _, _ = split_train_test(wind_histo_norm,\n",
    "                                                   time_list)\n",
    "\n",
    "    # aggregate datasets per time period (3D-ndarrays)\n",
    "    total_train = ndarray_to_3d(train_temp, train_prcp, train_wind)\n",
    "    total_test = ndarray_to_3d(test_temp, test_prcp, test_wind)\n",
    "    total_proj = ndarray_to_3d(temp_proj_norm, \n",
    "                               prcp_proj_norm, \n",
    "                               wind_proj_norm)\n",
    "\n",
    "    # save data in input folder\n",
    "    if saving_on == True:\n",
    "        np.save(\"../input/preprocessed_3d_train_data.npy\", total_train)\n",
    "        np.save(\"../input/preprocessed_3d_test_data.npy\", total_test)\n",
    "        np.save(\"../input/preprocessed_3d_proj_data.npy\", total_proj)\n",
    "        pd.DataFrame(train_time).to_csv('../input/dates_train_data.csv')\n",
    "        pd.DataFrame(test_time).to_csv('../input/dates_test_data.csv')\n",
    "        pd.DataFrame(time_proj).to_csv('../input/dates_proj_data.csv')\n",
    "        \n",
    "    return total_train, total_test, total_proj, train_time, test_time, time_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f78f1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train, total_test, total_proj, time_train, time_test, time_proj = preprocess_3d(temp_histo, temp_proj, prcp_histo,\n",
    "              prcp_proj, wind_histo, wind_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89141e9",
   "metadata": {},
   "source": [
    "#### 7. Step-by-Step Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ddfe094",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_histo_nd, time_list = xr_to_ndarray(temp_histo, sq32_west_europe)\n",
    "prcp_histo_nd, _ = xr_to_ndarray(prcp_histo, sq32_west_europe)\n",
    "wind_histo_nd, _ = xr_to_ndarray(wind_histo, sq32_west_europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ee0d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_proj_nd, time_proj = xr_to_ndarray(temp_proj, sq32_west_europe)\n",
    "prcp_proj_nd, _ = xr_to_ndarray(prcp_proj, sq32_west_europe)\n",
    "wind_proj_nd, _ = xr_to_ndarray(wind_proj, sq32_west_europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa7d142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_extrema = get_extrema(temp_histo_nd, temp_proj_nd)\n",
    "prcp_extrema = get_extrema(prcp_histo_nd, prcp_proj_nd)\n",
    "wind_extrema = get_extrema(wind_histo_nd, wind_proj_nd)\n",
    "extrema = np.array([temp_extrema, prcp_extrema, wind_extrema, temp_extrema, prcp_extrema, wind_extrema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e61c35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_histo_norm = normalize(temp_histo_nd, temp_extrema)\n",
    "temp_proj_norm = normalize(temp_proj_nd, temp_extrema)\n",
    "\n",
    "prcp_histo_norm = normalize(prcp_histo_nd, prcp_extrema)\n",
    "prcp_proj_norm = normalize(prcp_proj_nd, prcp_extrema)\n",
    "\n",
    "wind_histo_norm = normalize(wind_histo_nd, wind_extrema)\n",
    "wind_proj_norm = normalize(wind_proj_nd, wind_extrema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c520c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp, test_temp, train_time, test_time = split_train_test(temp_histo_norm,\n",
    "                                                                time_list)\n",
    "train_prcp, test_prcp, _, _ = split_train_test(prcp_histo_norm, \n",
    "                                               time_list)\n",
    "train_wind, test_wind, _, _ = split_train_test(wind_histo_norm,\n",
    "                                               time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ce3b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = ndarray_to_3d(train_temp, train_prcp, train_wind)\n",
    "total_test = ndarray_to_3d(test_temp, test_prcp, test_wind)\n",
    "total_proj = ndarray_to_3d(temp_proj_norm, \n",
    "                           prcp_proj_norm, \n",
    "                           wind_proj_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb684f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../input/preprocessed_3d_train_data.npy\", total_train)\n",
    "np.save(\"../input/preprocessed_3d_test_data.npy\", total_test)\n",
    "np.save(\"../input/preprocessed_3d_proj_data.npy\", total_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092eab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_time).to_csv('../input/dates_train_data.csv')\n",
    "pd.DataFrame(test_time).to_csv('../input/dates_test_data.csv')\n",
    "pd.DataFrame(time_proj).to_csv('../input/dates_proj_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
